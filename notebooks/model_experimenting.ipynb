{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shite  i dont think theyre very happy</td>\n",
       "      <td>i dont think theyre very happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>if you want to understand animals i mean i mea...</td>\n",
       "      <td>if you want to understand game i mean really u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sonia that smells awful</td>\n",
       "      <td>sonio that is a terrible thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam dolans father is out of his mind</td>\n",
       "      <td>sam dolans father checks out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theres a guy out there whos fucking serious</td>\n",
       "      <td>this is one very serious boy we have out there</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0              shite  i dont think theyre very happy   \n",
       "1  if you want to understand animals i mean i mea...   \n",
       "2                            sonia that smells awful   \n",
       "3               sam dolans father is out of his mind   \n",
       "4        theres a guy out there whos fucking serious   \n",
       "\n",
       "                                         translation  \n",
       "0                     i dont think theyre very happy  \n",
       "1  if you want to understand game i mean really u...  \n",
       "2                     sonio that is a terrible thing  \n",
       "3                       sam dolans father checks out  \n",
       "4     this is one very serious boy we have out there  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/interim/training_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference      0\n",
       "translation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic = df['reference'].tolist()\n",
    "nontoxic = df['translation'].tolist()\n",
    "# df['translation'].\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Let's assume `toxic` and `nontoxic` are lists of sentences (strings).\n",
    "all_texts = toxic + nontoxic  # Combine both lists for creating the vocab.\n",
    "# print(all_texts[0].split())\n",
    "# # Tokenize the text\n",
    "# tokenized_texts = []\n",
    "# for i, sentence in enumerate(nontoxic):\n",
    "#     try:\n",
    "#         tokenized_texts.append(sentence.split())\n",
    "#     except:\n",
    "#         print(i, sentence)\n",
    "tokenized_texts = [sentence.split() for sentence in all_texts]\n",
    "\n",
    "# Flatten the list of token lists into a single list of tokens\n",
    "all_tokens = [token for sublist in tokenized_texts for token in sublist]\n",
    "\n",
    "# Count the frequency of tokens in the corpus\n",
    "token_freqs = Counter(all_tokens)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = {\n",
    "    '<pad>': 0,\n",
    "    '<sos>': 1,\n",
    "    '<eos>': 2,\n",
    "    '<unk>': 3,  # Tokens not found in the vocab will be replaced with <unk>\n",
    "}\n",
    "\n",
    "# Start the index count from 4, as 0-3 are reserved for special tokens\n",
    "for index, token in enumerate(token_freqs, start=4):\n",
    "    vocab[token] = index\n",
    "\n",
    "# Now, vocab is a dictionary mapping each token to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Example of a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, toxic_texts, nontoxic_texts, vocab):\n",
    "        self.toxic_texts = toxic_texts\n",
    "        self.nontoxic_texts = nontoxic_texts\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.toxic_texts)\n",
    "\n",
    "    def vectorize(self, text):\n",
    "        return [self.vocab[token] for token in text.split()]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toxic_vectorized = self.vectorize(self.toxic_texts[idx])\n",
    "        nontoxic_vectorized = self.vectorize(self.nontoxic_texts[idx])\n",
    "        return torch.tensor(toxic_vectorized), torch.tensor(nontoxic_vectorized)\n",
    "\n",
    "\n",
    "# Assume 'vocab' is a dictionary mapping tokens to indices, and 'toxic' and 'nontoxic' are lists of sentences.\n",
    "dataset = TextDataset(toxic, nontoxic, vocab)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    toxic_list, nontoxic_list = [], []\n",
    "    for toxic, nontoxic in batch:\n",
    "        toxic_list.append(torch.tensor(toxic, requires_grad=True))\n",
    "        nontoxic_list.append(torch.tensor(nontoxic, requires_grad=True))\n",
    "    return pad_sequence(toxic_list, padding_value=vocab[\"<pad>\"]), pad_sequence(\n",
    "        nontoxic_list, padding_value=vocab[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from numpy import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "# device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\"\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/7myxq8l12wz8s_qhqdqq6d4r0000gn/T/ipykernel_51351/1646335741.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  toxic_list.append(torch.tensor(toxic, requires_grad=True))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m src, trg \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         src, trg \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m toxic_list, nontoxic_list \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m toxic, nontoxic \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     toxic_list\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mtensor(toxic, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     nontoxic_list\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(nontoxic, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pad_sequence(toxic_list, padding_value\u001b[39m=\u001b[39mvocab[\u001b[39m\"\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m\"\u001b[39m]), pad_sequence(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     nontoxic_list, padding_value\u001b[39m=\u001b[39mvocab[\u001b[39m\"\u001b[39m\u001b[39m<pad>\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/random/Data/Innopolis/pmldl/assignment1/notebooks/model_experimenting.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for src, trg in loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        print(output.shape)\n",
    "        print(trg.shape)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(epoch, loss.value())\n",
    "    print(epoch, loss.value())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
